{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# plotting import\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_one(X):\n",
    "    \n",
    "    \"\"\"prepend a one vector to X.\"\"\"\n",
    "    \n",
    "    # get number of training points\n",
    "    n_trains = X.shape[0]       \n",
    "    # creat a column vector (dim = nx1) with values of 1s only.\n",
    "    ones = np.ones(X.shape[0])\n",
    "    \n",
    "    # create new data matrix of linear features.\n",
    "    phi_X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    return phi_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP for learning Boolean AND and  XOR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.35762297e-14 4.53978687e-05 4.53978687e-05 9.99954602e-01]\n"
     ]
    }
   ],
   "source": [
    "#input for AND function\n",
    "\n",
    "X = prepend_one(np.array( [[0,0],[0,1],[1,0],[1,1]]))\n",
    "\n",
    "w = [-30.0,20.0,20.0]\n",
    "\n",
    "y = sigmoid(np.dot(X,w))\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.54391049e-05 9.99954520e-01 9.99954520e-01 4.54391049e-05]\n"
     ]
    }
   ],
   "source": [
    "# XOR function: using a 2-layered NN to estimate XOR (with sigmoid activations at hidden nodes and output)\n",
    "\n",
    "# augment bias term into inputs\n",
    "X = prepend_one(np.array( [[0,0],[0,1],[1,0],[1,1]]))\n",
    "\n",
    "# weights of the first layer (3 hidden nodes in which one bias term)\n",
    "w1 = np.array([[-30.0,20.0,20.0],[10,-20,-20]]).T\n",
    "# weights of the second layer (one output node)\n",
    "w2 = [10,-20,-20]\n",
    "\n",
    "\n",
    "# forward progpagation\n",
    "g = prepend_one(sigmoid(np.dot(X,w1)))\n",
    "\n",
    "y = sigmoid(np.dot(g,w2))\n",
    "\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Backpropagation code\n",
    "## This is code is from http://cs229.stanford.edu by Andrew Ng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights before update:\n",
      "b1  :  [[0.07965221]\n",
      " [0.21098417]]\n",
      "b2  :  [[0.18303908]]\n",
      "W1  :  [[0.59912385 0.04753944]\n",
      " [0.29077768 0.98248603]]\n",
      "W2  :  [[0.56490147 0.50417534]]\n",
      "loss before update:  [[1.17211337]]\n",
      "\n",
      "weights after update:\n",
      "b1  :  [[0.07758436]\n",
      " [0.20922561]]\n",
      "b2  :  [[0.16828138]]\n",
      "W1  :  [[0.59882369 0.04707027]\n",
      " [0.29052242 0.98208703]]\n",
      "W2  :  [[0.55686982 0.49507239]]\n",
      "loss after 1 update:  [[1.15473766]]\n",
      "MSE loss:  [[1.07003859]]  after iteration:  0\n",
      "MSE loss:  [[0.98929773]]  after iteration:  1\n",
      "MSE loss:  [[0.91335118]]  after iteration:  2\n",
      "MSE loss:  [[0.84282065]]  after iteration:  3\n",
      "MSE loss:  [[0.77807358]]  after iteration:  4\n",
      "MSE loss:  [[0.71922051]]  after iteration:  5\n",
      "MSE loss:  [[0.66614566]]  after iteration:  6\n",
      "MSE loss:  [[0.61855878]]  after iteration:  7\n",
      "MSE loss:  [[0.57605397]]  after iteration:  8\n",
      "MSE loss:  [[0.5381641]]  after iteration:  9\n",
      "MSE loss:  [[0.50440408]]  after iteration:  10\n",
      "MSE loss:  [[0.47430139]]  after iteration:  11\n",
      "MSE loss:  [[0.44741486]]  after iteration:  12\n",
      "MSE loss:  [[0.42334437]]  after iteration:  13\n",
      "MSE loss:  [[0.40173417]]  after iteration:  14\n",
      "MSE loss:  [[0.38227227]]  after iteration:  15\n",
      "MSE loss:  [[0.36468744]]  after iteration:  16\n",
      "MSE loss:  [[0.34874522]]  after iteration:  17\n",
      "MSE loss:  [[0.33424357]]  after iteration:  18\n",
      "MSE loss:  [[0.32100859]]  after iteration:  19\n",
      "MSE loss:  [[0.30889062]]  after iteration:  20\n",
      "MSE loss:  [[0.29776072]]  after iteration:  21\n",
      "MSE loss:  [[0.2875076]]  after iteration:  22\n",
      "MSE loss:  [[0.27803503]]  after iteration:  23\n",
      "MSE loss:  [[0.26925954]]  after iteration:  24\n",
      "MSE loss:  [[0.26110858]]  after iteration:  25\n",
      "MSE loss:  [[0.25351887]]  after iteration:  26\n",
      "MSE loss:  [[0.24643504]]  after iteration:  27\n",
      "MSE loss:  [[0.23980855]]  after iteration:  28\n",
      "MSE loss:  [[0.23359666]]  after iteration:  29\n",
      "MSE loss:  [[0.22776164]]  after iteration:  30\n",
      "MSE loss:  [[0.2222701]]  after iteration:  31\n",
      "MSE loss:  [[0.21709241]]  after iteration:  32\n",
      "MSE loss:  [[0.21220214]]  after iteration:  33\n",
      "MSE loss:  [[0.20757575]]  after iteration:  34\n",
      "MSE loss:  [[0.20319213]]  after iteration:  35\n",
      "MSE loss:  [[0.19903235]]  after iteration:  36\n",
      "MSE loss:  [[0.19507937]]  after iteration:  37\n",
      "MSE loss:  [[0.19131782]]  after iteration:  38\n",
      "MSE loss:  [[0.1877338]]  after iteration:  39\n",
      "MSE loss:  [[0.18431474]]  after iteration:  40\n",
      "MSE loss:  [[0.1810492]]  after iteration:  41\n",
      "MSE loss:  [[0.17792677]]  after iteration:  42\n",
      "MSE loss:  [[0.17493798]]  after iteration:  43\n",
      "MSE loss:  [[0.17207416]]  after iteration:  44\n",
      "MSE loss:  [[0.16932737]]  after iteration:  45\n",
      "MSE loss:  [[0.16669034]]  after iteration:  46\n",
      "MSE loss:  [[0.1641564]]  after iteration:  47\n",
      "MSE loss:  [[0.16171938]]  after iteration:  48\n",
      "MSE loss:  [[0.15937363]]  after iteration:  49\n",
      "MSE loss:  [[0.15711392]]  after iteration:  50\n",
      "MSE loss:  [[0.1549354]]  after iteration:  51\n",
      "MSE loss:  [[0.1528336]]  after iteration:  52\n",
      "MSE loss:  [[0.15080437]]  after iteration:  53\n",
      "MSE loss:  [[0.14884386]]  after iteration:  54\n",
      "MSE loss:  [[0.14694847]]  after iteration:  55\n",
      "MSE loss:  [[0.14511486]]  after iteration:  56\n",
      "MSE loss:  [[0.14333994]]  after iteration:  57\n",
      "MSE loss:  [[0.14162079]]  after iteration:  58\n",
      "MSE loss:  [[0.13995471]]  after iteration:  59\n",
      "MSE loss:  [[0.13833915]]  after iteration:  60\n",
      "MSE loss:  [[0.13677174]]  after iteration:  61\n",
      "MSE loss:  [[0.13525026]]  after iteration:  62\n",
      "MSE loss:  [[0.1337726]]  after iteration:  63\n",
      "MSE loss:  [[0.13233682]]  after iteration:  64\n",
      "MSE loss:  [[0.13094105]]  after iteration:  65\n",
      "MSE loss:  [[0.12958357]]  after iteration:  66\n",
      "MSE loss:  [[0.12826274]]  after iteration:  67\n",
      "MSE loss:  [[0.12697701]]  after iteration:  68\n",
      "MSE loss:  [[0.12572493]]  after iteration:  69\n",
      "MSE loss:  [[0.12450512]]  after iteration:  70\n",
      "MSE loss:  [[0.12331628]]  after iteration:  71\n",
      "MSE loss:  [[0.12215718]]  after iteration:  72\n",
      "MSE loss:  [[0.12102665]]  after iteration:  73\n",
      "MSE loss:  [[0.1199236]]  after iteration:  74\n",
      "MSE loss:  [[0.11884697]]  after iteration:  75\n",
      "MSE loss:  [[0.11779577]]  after iteration:  76\n",
      "MSE loss:  [[0.11676906]]  after iteration:  77\n",
      "MSE loss:  [[0.11576594]]  after iteration:  78\n",
      "MSE loss:  [[0.11478557]]  after iteration:  79\n",
      "MSE loss:  [[0.11382712]]  after iteration:  80\n",
      "MSE loss:  [[0.11288983]]  after iteration:  81\n",
      "MSE loss:  [[0.11197297]]  after iteration:  82\n",
      "MSE loss:  [[0.11107583]]  after iteration:  83\n",
      "MSE loss:  [[0.11019774]]  after iteration:  84\n",
      "MSE loss:  [[0.10933806]]  after iteration:  85\n",
      "MSE loss:  [[0.1084962]]  after iteration:  86\n",
      "MSE loss:  [[0.10767155]]  after iteration:  87\n",
      "MSE loss:  [[0.10686358]]  after iteration:  88\n",
      "MSE loss:  [[0.10607174]]  after iteration:  89\n",
      "MSE loss:  [[0.10529552]]  after iteration:  90\n",
      "MSE loss:  [[0.10453445]]  after iteration:  91\n",
      "MSE loss:  [[0.10378805]]  after iteration:  92\n",
      "MSE loss:  [[0.10305587]]  after iteration:  93\n",
      "MSE loss:  [[0.10233749]]  after iteration:  94\n",
      "MSE loss:  [[0.10163249]]  after iteration:  95\n",
      "MSE loss:  [[0.10094048]]  after iteration:  96\n",
      "MSE loss:  [[0.10026108]]  after iteration:  97\n",
      "MSE loss:  [[0.09959392]]  after iteration:  98\n",
      "MSE loss:  [[0.09893866]]  after iteration:  99\n"
     ]
    }
   ],
   "source": [
    "# This is code is from http://cs229.stanford.edu by Andrew Ng\n",
    "# Backpropagation code\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "\n",
    "# Example backpropagation code for binary classification with 2-layer\n",
    "# neural network (single hidden layer)\n",
    "\n",
    "\n",
    "\n",
    "# forward pass: receiving input x and return output y\n",
    "def fprop(x, y, params):\n",
    "    # Follows procedure given in notessigmoid\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = -(y * np.log(a2) + (1-y) * np.log(1-a2))\n",
    "    ret = {'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss}\n",
    "    for key in params:        \n",
    "        ret[key] = params[key]   \n",
    "    return ret\n",
    "\n",
    "#backward pass: return updated weights\n",
    "def bprop(fprop_cache):\n",
    "    # Follows procedure given in notes\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    de2 = - a2*(1-a2)*(y - a2)\n",
    "    dW2 = np.dot(de2, a1.T)\n",
    "    db2 = de2\n",
    "    de1 = np.dot(fprop_cache['W2'].T, de2) * a1 * (1-a1)\n",
    "    dW1 = np.dot(de1, x.T)\n",
    "    db1 = de1\n",
    "    return {'b1': db1, 'W1': dW1, 'b2': db2, 'W2': dW2}\n",
    "\n",
    "# main code\n",
    "if __name__ == '__main__':\n",
    "    # Initialize random parameters/weights and inputs\n",
    "    W1 = np.random.rand(2,2)\n",
    "    b1 = np.random.rand(2, 1)\n",
    "    W2 = np.random.rand(1, 2)\n",
    "    b2 = np.random.rand(1, 1)\n",
    "    params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    x = np.random.rand(2, 1)\n",
    "    y = np.random.randint(0, 2)  # Returns 0/1\n",
    "\n",
    "    fprop_cache = fprop(x, y, params)\n",
    "    #backpropagation (to compute gradients for one iteration: at current x,y) \n",
    "    bprop_cache = bprop(fprop_cache)\n",
    "   \n",
    "        \n",
    "    print(\"weights before update:\")\n",
    "    for key in params:        \n",
    "        print(key,\" : \",params[key])\n",
    "    fprop_cache=fprop(x,y,params)\n",
    "    print(\"loss before update: \", fprop_cache['loss'])\n",
    "    print()\n",
    "    print(\"weights after update:\")\n",
    "    for key in params:\n",
    "        params[key] = params[key] - 0.1 * bprop_cache[key] \n",
    "        print(key,\" : \",params[key])\n",
    "    fprop_cache=fprop(x,y,params)\n",
    "    print(\"loss after 1 update: \", fprop_cache['loss'])\n",
    "    \n",
    "    ############### do more updates ###########################\n",
    "    \n",
    "    for iteration in range(100):\n",
    "        fprop_cache = fprop(x, y, params)\n",
    "        #backpropagation (to compute gradients for one iteration: at current x,y) \n",
    "        bprop_cache = bprop(fprop_cache)\n",
    "        for key in params:\n",
    "            params[key] = params[key] - 0.5 * bprop_cache[key] \n",
    "        \n",
    "        fprop_cache=fprop(x,y,params)\n",
    "        print(\"MSE loss: \", fprop_cache['loss'], \" after iteration: \",iteration)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
